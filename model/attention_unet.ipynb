{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sdqBMkWdCHEZ"},"outputs":[],"source":["%%capture\n","!pip install torchsummary"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27570,"status":"ok","timestamp":1692922359887,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"},"user_tz":-600},"id":"wBKR5yplBOFP","outputId":"45c2d577-4aaa-4720-a702-44b9ed6dad54"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 1, 256, 256])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 256, 256]           1,792\n","       BatchNorm2d-2         [-1, 64, 256, 256]             128\n","              ReLU-3         [-1, 64, 256, 256]               0\n","            Conv2d-4         [-1, 64, 256, 256]          36,928\n","       BatchNorm2d-5         [-1, 64, 256, 256]             128\n","              ReLU-6         [-1, 64, 256, 256]               0\n","        conv_block-7         [-1, 64, 256, 256]               0\n","         MaxPool2d-8         [-1, 64, 128, 128]               0\n","     encoder_block-9  [[-1, 64, 256, 256], [-1, 64, 128, 128]]               0\n","           Conv2d-10        [-1, 128, 128, 128]          73,856\n","      BatchNorm2d-11        [-1, 128, 128, 128]             256\n","             ReLU-12        [-1, 128, 128, 128]               0\n","           Conv2d-13        [-1, 128, 128, 128]         147,584\n","      BatchNorm2d-14        [-1, 128, 128, 128]             256\n","             ReLU-15        [-1, 128, 128, 128]               0\n","       conv_block-16        [-1, 128, 128, 128]               0\n","        MaxPool2d-17          [-1, 128, 64, 64]               0\n","    encoder_block-18  [[-1, 128, 128, 128], [-1, 128, 64, 64]]               0\n","           Conv2d-19          [-1, 256, 64, 64]         295,168\n","      BatchNorm2d-20          [-1, 256, 64, 64]             512\n","             ReLU-21          [-1, 256, 64, 64]               0\n","           Conv2d-22          [-1, 256, 64, 64]         590,080\n","      BatchNorm2d-23          [-1, 256, 64, 64]             512\n","             ReLU-24          [-1, 256, 64, 64]               0\n","       conv_block-25          [-1, 256, 64, 64]               0\n","        MaxPool2d-26          [-1, 256, 32, 32]               0\n","    encoder_block-27  [[-1, 256, 64, 64], [-1, 256, 32, 32]]               0\n","           Conv2d-28          [-1, 512, 32, 32]       1,180,160\n","      BatchNorm2d-29          [-1, 512, 32, 32]           1,024\n","             ReLU-30          [-1, 512, 32, 32]               0\n","           Conv2d-31          [-1, 512, 32, 32]       2,359,808\n","      BatchNorm2d-32          [-1, 512, 32, 32]           1,024\n","             ReLU-33          [-1, 512, 32, 32]               0\n","       conv_block-34          [-1, 512, 32, 32]               0\n","         Upsample-35          [-1, 512, 64, 64]               0\n","           Conv2d-36          [-1, 256, 64, 64]         131,328\n","      BatchNorm2d-37          [-1, 256, 64, 64]             512\n","           Conv2d-38          [-1, 256, 64, 64]          65,792\n","      BatchNorm2d-39          [-1, 256, 64, 64]             512\n","             ReLU-40          [-1, 256, 64, 64]               0\n","           Conv2d-41          [-1, 256, 64, 64]          65,792\n","          Sigmoid-42          [-1, 256, 64, 64]               0\n","   attention_gate-43          [-1, 256, 64, 64]               0\n","           Conv2d-44          [-1, 256, 64, 64]       1,769,728\n","      BatchNorm2d-45          [-1, 256, 64, 64]             512\n","             ReLU-46          [-1, 256, 64, 64]               0\n","           Conv2d-47          [-1, 256, 64, 64]         590,080\n","      BatchNorm2d-48          [-1, 256, 64, 64]             512\n","             ReLU-49          [-1, 256, 64, 64]               0\n","       conv_block-50          [-1, 256, 64, 64]               0\n","    decoder_block-51          [-1, 256, 64, 64]               0\n","         Upsample-52        [-1, 256, 128, 128]               0\n","           Conv2d-53        [-1, 128, 128, 128]          32,896\n","      BatchNorm2d-54        [-1, 128, 128, 128]             256\n","           Conv2d-55        [-1, 128, 128, 128]          16,512\n","      BatchNorm2d-56        [-1, 128, 128, 128]             256\n","             ReLU-57        [-1, 128, 128, 128]               0\n","           Conv2d-58        [-1, 128, 128, 128]          16,512\n","          Sigmoid-59        [-1, 128, 128, 128]               0\n","   attention_gate-60        [-1, 128, 128, 128]               0\n","           Conv2d-61        [-1, 128, 128, 128]         442,496\n","      BatchNorm2d-62        [-1, 128, 128, 128]             256\n","             ReLU-63        [-1, 128, 128, 128]               0\n","           Conv2d-64        [-1, 128, 128, 128]         147,584\n","      BatchNorm2d-65        [-1, 128, 128, 128]             256\n","             ReLU-66        [-1, 128, 128, 128]               0\n","       conv_block-67        [-1, 128, 128, 128]               0\n","    decoder_block-68        [-1, 128, 128, 128]               0\n","         Upsample-69        [-1, 128, 256, 256]               0\n","           Conv2d-70         [-1, 64, 256, 256]           8,256\n","      BatchNorm2d-71         [-1, 64, 256, 256]             128\n","           Conv2d-72         [-1, 64, 256, 256]           4,160\n","      BatchNorm2d-73         [-1, 64, 256, 256]             128\n","             ReLU-74         [-1, 64, 256, 256]               0\n","           Conv2d-75         [-1, 64, 256, 256]           4,160\n","          Sigmoid-76         [-1, 64, 256, 256]               0\n","   attention_gate-77         [-1, 64, 256, 256]               0\n","           Conv2d-78         [-1, 64, 256, 256]         110,656\n","      BatchNorm2d-79         [-1, 64, 256, 256]             128\n","             ReLU-80         [-1, 64, 256, 256]               0\n","           Conv2d-81         [-1, 64, 256, 256]          36,928\n","      BatchNorm2d-82         [-1, 64, 256, 256]             128\n","             ReLU-83         [-1, 64, 256, 256]               0\n","       conv_block-84         [-1, 64, 256, 256]               0\n","    decoder_block-85         [-1, 64, 256, 256]               0\n","           Conv2d-86          [-1, 1, 256, 256]              65\n","================================================================\n","Total params: 8,135,745\n","Trainable params: 8,135,745\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.75\n","Forward/backward pass size (MB): 44038749.50\n","Params size (MB): 31.04\n","Estimated Total Size (MB): 44038781.29\n","----------------------------------------------------------------\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class conv_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_c),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class encoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.conv = conv_block(in_c, out_c)\n","        self.pool = nn.MaxPool2d((2, 2))\n","\n","    def forward(self, x):\n","        s = self.conv(x)\n","        p = self.pool(s)\n","        return s, p\n","\n","class attention_gate(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.Wg = nn.Sequential(\n","            nn.Conv2d(in_c[0], out_c, kernel_size=1, padding=0),\n","            nn.BatchNorm2d(out_c)\n","        )\n","        self.Ws = nn.Sequential(\n","            nn.Conv2d(in_c[1], out_c, kernel_size=1, padding=0),\n","            nn.BatchNorm2d(out_c)\n","        )\n","        self.relu = nn.ReLU(inplace=True)\n","        self.output = nn.Sequential(\n","            nn.Conv2d(out_c, out_c, kernel_size=1, padding=0),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, g, s):\n","        Wg = self.Wg(g)\n","        Ws = self.Ws(s)\n","        out = self.relu(Wg + Ws)\n","        out = self.output(out)\n","        return out * s\n","\n","class decoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","\n","        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        self.ag = attention_gate(in_c, out_c)\n","        self.c1 = conv_block(in_c[0]+out_c, out_c)\n","\n","    def forward(self, x, s):\n","        x = self.up(x)\n","        s = self.ag(x, s)\n","        x = torch.cat([x, s], axis=1)\n","        x = self.c1(x)\n","        return x\n","\n","class attention_unet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.e1 = encoder_block(3, 64)\n","        self.e2 = encoder_block(64, 128)\n","        self.e3 = encoder_block(128, 256)\n","\n","        self.b1 = conv_block(256, 512)\n","\n","        self.d1 = decoder_block([512, 256], 256)\n","        self.d2 = decoder_block([256, 128], 128)\n","        self.d3 = decoder_block([128, 64], 64)\n","\n","        self.output = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n","\n","    def forward(self, x):\n","        s1, p1 = self.e1(x)\n","        s2, p2 = self.e2(p1)\n","        s3, p3 = self.e3(p2)\n","\n","        b1 = self.b1(p3)\n","\n","        d1 = self.d1(b1, s3)\n","        d2 = self.d2(d1, s2)\n","        d3 = self.d3(d2, s1)\n","\n","        output = self.output(d3)\n","        return output\n","\n","class TverskyLoss(nn.Module):\n","    def __init__(self, alpha=0.5, beta=0.5):\n","        super(TverskyLoss, self).__init__()\n","        self.alpha = alpha\n","        self.beta = beta\n","\n","    def forward(self, input, target):\n","        smooth = 1e-5\n","        input = torch.sigmoid(input)\n","\n","        # Flatten input and target tensors\n","        input = input.view(-1)\n","        target = target.view(-1)\n","\n","        # True Positives, False Positives & False Negatives\n","        TP = (input * target).sum()\n","        FP = ((1-target) * input).sum()\n","        FN = (target * (1-input)).sum()\n","\n","        Tversky = (TP + smooth) / (TP + self.alpha*FP + self.beta*FN + smooth)\n","\n","        return 1 - Tversky\n","\n","if __name__ == \"__main__\":\n","    x = torch.randn((8, 3, 256, 256))\n","    model = attention_unet()\n","    output = model(x)\n","    print(output.shape)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOb+89v3yeCelEZ+e6Kcz85","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
