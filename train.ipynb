{"cells":[{"cell_type":"markdown","metadata":{"id":"LnCNsTq9994Y"},"source":["# Install Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"eYjhuUq2I44M","executionInfo":{"status":"ok","timestamp":1692870761959,"user_tz":-600,"elapsed":11490,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"}}},"outputs":[],"source":["%%capture\n","!pip install monai\n","!pip install dicom2nifti"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45761,"status":"ok","timestamp":1692870872498,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"},"user_tz":-600},"id":"zsV5Adna65oI","outputId":"bf44e3ac-45b5-4da7-e39d-881fe89279cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Wg3TQ8Zia0s7","executionInfo":{"status":"ok","timestamp":1692870877676,"user_tz":-600,"elapsed":5184,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"}}},"outputs":[],"source":["!cp /content/drive/MyDrive/Machine-Learning-Biomedicine/PankVision-3D/preprocess/final_preprocess.py /content\n","!cp /content/drive/MyDrive/Machine-Learning-Biomedicine/PankVision-3D/utilities/final_utilities.py /content\n","!cp /content/drive/MyDrive/Machine-Learning-Biomedicine/PankVision-3D/model/get_model.py /content"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"SUFEgO04I1Gl","executionInfo":{"status":"ok","timestamp":1692873071281,"user_tz":-600,"elapsed":1011,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"}}},"outputs":[],"source":["from monai.networks.nets import UNet\n","from monai.networks.layers import Norm\n","from monai.losses import DiceLoss, DiceCELoss\n","\n","import torch\n","from final_utilities import train\n","from get_model import get_model\n","from final_preprocess import preprocess_image, prepare\n","\n","import numpy as np\n","from skimage import exposure\n","from monai.transforms import (\n","    Compose,\n","    LoadImaged,\n","    AddChanneld,\n","    Spacingd,\n","    Orientationd,\n","    ScaleIntensityRanged,\n","    CropForegroundd,\n","    Resized,\n","    ToTensord,\n",")\n","from monai.data import CacheDataset, Dataset, DataLoader\n","from glob import glob\n","import os\n","from monai.utils import set_determinism"]},{"cell_type":"code","source":["from monai.utils import first\n","import matplotlib.pyplot as plt\n","import torch\n","import os\n","import numpy as np\n","from monai.losses import DiceLoss\n","from tqdm import tqdm\n","from sklearn.metrics import confusion_matrix\n","\n","def dice_metric(predicted, target):\n","    '''\n","    In this function we take `predicted` and `target` (label) to calculate the dice coeficient then we use it\n","    to calculate a metric value for the training and the validation.\n","    '''\n","    dice_value = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n","    value = 1 - dice_value(predicted, target).item()\n","    return value\n","\n","def calculate_weights(val1, val2):\n","    '''\n","    In this function we take the number of the background and the forgroud pixels to return the `weights`\n","    for the cross entropy loss values.\n","    '''\n","    count = np.array([val1, val2])\n","    summ = count.sum()\n","    weights = count/summ\n","    weights = 1/weights\n","    summ = weights.sum()\n","    weights = weights/summ\n","    return torch.tensor(weights, dtype=torch.float32)\n","\n","def train(model, data_in, loss, optim, max_epochs, model_dir, test_interval=1 , device=torch.device(\"cuda:0\")):\n","    best_metric = -1\n","    best_metric_epoch = -1\n","    save_loss_train = []\n","    save_loss_test = []\n","    save_metric_train = []\n","    save_metric_test = []\n","    train_loader, test_loader = data_in\n","\n","    for epoch in range(max_epochs):\n","        print(\"-\" * 10)\n","        print(f\"epoch {epoch + 1}/{max_epochs}\")\n","        model.train()\n","        train_epoch_loss = 0\n","        train_step = 0\n","        epoch_metric_train = 0\n","        for batch_data in train_loader:\n","\n","            train_step += 1\n","\n","            volume = batch_data[\"vol\"]\n","            label = batch_data[\"seg\"]\n","            label = label != 0\n","            volume, label = (volume.to(device), label.to(device))\n","\n","            optim.zero_grad()\n","            outputs = model(volume)\n","\n","            train_loss = loss(outputs, label)\n","\n","            train_loss.backward()\n","            optim.step()\n","\n","            train_epoch_loss += train_loss.item()\n","            print(\n","                f\"{train_step}/{len(train_loader) // train_loader.batch_size}, \"\n","                f\"Train_loss: {train_loss.item():.4f}\")\n","\n","            train_metric = dice_metric(outputs, label)\n","            epoch_metric_train += train_metric\n","            print(f'Train_dice: {train_metric:.4f}')\n","\n","        print('-'*20)\n","\n","        train_epoch_loss /= train_step\n","        print(f'Epoch_loss: {train_epoch_loss:.4f}')\n","        save_loss_train.append(train_epoch_loss)\n","        np.save(os.path.join(model_dir, 'loss_train.npy'), save_loss_train)\n","\n","        epoch_metric_train /= train_step\n","        print(f'Epoch_metric: {epoch_metric_train:.4f}')\n","\n","        save_metric_train.append(epoch_metric_train)\n","        np.save(os.path.join(model_dir, 'metric_train.npy'), save_metric_train)\n","\n","        if test_interval > 0 and (epoch + 1) % test_interval == 0:\n","            model.eval()\n","            with torch.no_grad():\n","                test_epoch_loss = 0\n","                test_metric = 0\n","                epoch_metric_test = 0\n","                test_step = 0\n","\n","                all_preds = []\n","                all_labels = []\n","                for test_data in test_loader:\n","                    test_step += 1\n","                    test_volume = test_data[\"vol\"]\n","                    test_label = test_data[\"seg\"]\n","                    test_label = test_label != 0\n","                    test_volume, test_label = (test_volume.to(device), test_label.to(device),)\n","\n","                    test_outputs = model(test_volume)\n","\n","                    # Calculate test loss\n","                    test_loss = loss(test_outputs, test_label)\n","                    test_epoch_loss += test_loss.item()\n","\n","                    # Calculate test dice metric\n","                    test_metric = dice_metric(test_outputs, test_label)\n","                    epoch_metric_test += test_metric\n","\n","                    # Get predicted pixel-wise labels\n","                    _, preds = torch.max(test_outputs, dim=1)\n","                    all_preds.extend(preds.cpu().numpy().flatten())  # Flatten predicted labels\n","                    all_labels.extend(test_label.cpu().numpy().flatten())  # Flatten ground truth labels\n","\n","                # Print test loss and test metric\n","                test_epoch_loss /= test_step\n","                print(f'test_loss_epoch: {test_epoch_loss:.4f}')\n","\n","                epoch_metric_test /= test_step\n","                print(f'test_dice_epoch: {epoch_metric_test:.4f}')\n","\n","                # Calculate and print confusion matrix\n","                cm = confusion_matrix(all_labels, all_preds)\n","                print('Confusion Matrix:')\n","                print(cm)\n","\n","                test_epoch_loss /= test_step\n","                print(f'test_loss_epoch: {test_epoch_loss:.4f}')\n","                save_loss_test.append(test_epoch_loss)\n","                np.save(os.path.join(model_dir, 'loss_test.npy'), save_loss_test)\n","\n","                epoch_metric_test /= test_step\n","                print(f'test_dice_epoch: {epoch_metric_test:.4f}')\n","                save_metric_test.append(epoch_metric_test)\n","                np.save(os.path.join(model_dir, 'metric_test.npy'), save_metric_test)\n","\n","                if epoch_metric_test > best_metric:\n","                    best_metric = epoch_metric_test\n","                    best_metric_epoch = epoch + 1\n","                    torch.save(model.state_dict(), os.path.join(\n","                        model_dir, \"best_metric_model.pth\"))\n","\n","                print(\n","                    f\"current epoch: {epoch + 1} current mean dice: {test_metric:.4f}\"\n","                    f\"\\nbest mean dice: {best_metric:.4f} \"\n","                    f\"at epoch: {best_metric_epoch}\"\n","                )\n","\n","    print(\n","        f\"train completed, best_metric: {best_metric:.4f} \"\n","        f\"at epoch: {best_metric_epoch}\")\n","\n","\n","\n","def show_patient(data, SLICE_NUMBER=1, train=True, test=False):\n","    \"\"\"\n","    This function is to show one patient from your datasets, so that you can si if the it is okay or you need\n","    to change/delete something.\n","\n","    `data`: this parameter should take the patients from the data loader, which means you need to can the function\n","    prepare first and apply the transforms that you want after that pass it to this function so that you visualize\n","    the patient with the transforms that you want.\n","    `SLICE_NUMBER`: this parameter will take the slice number that you want to display/show\n","    `train`: this parameter is to say that you want to display a patient from the training data (by default it is true)\n","    `test`: this parameter is to say that you want to display a patient from the testing patients.\n","    \"\"\"\n","\n","    check_patient_train, check_patient_test = data\n","\n","    view_train_patient = first(check_patient_train)\n","    view_test_patient = first(check_patient_test)\n","\n","\n","    if train:\n","        plt.figure(\"Visualization Train\", (12, 6))\n","        plt.subplot(1, 2, 1)\n","        plt.title(f\"vol {SLICE_NUMBER}\")\n","        plt.imshow(view_train_patient[\"vol\"][0, 0, :, :, SLICE_NUMBER], cmap=\"gray\")\n","\n","        plt.subplot(1, 2, 2)\n","        plt.title(f\"seg {SLICE_NUMBER}\")\n","        plt.imshow(view_train_patient[\"seg\"][0, 0, :, :, SLICE_NUMBER])\n","        plt.show()\n","\n","    if test:\n","        plt.figure(\"Visualization Test\", (12, 6))\n","        plt.subplot(1, 2, 1)\n","        plt.title(f\"vol {SLICE_NUMBER}\")\n","        plt.imshow(view_test_patient[\"vol\"][0, 0, :, :, SLICE_NUMBER], cmap=\"gray\")\n","\n","        plt.subplot(1, 2, 2)\n","        plt.title(f\"seg {SLICE_NUMBER}\")\n","        plt.imshow(view_test_patient[\"seg\"][0, 0, :, :, SLICE_NUMBER])\n","        plt.show()\n","\n","\n","def calculate_pixels(data):\n","    val = np.zeros((1, 2))\n","\n","    for batch in tqdm(data):\n","        batch_label = batch[\"seg\"] != 0\n","        _, count = np.unique(batch_label, return_counts=True)\n","\n","        if len(count) == 1:\n","            count = np.append(count, 0)\n","        val += count\n","\n","    print('The last values:', val)\n","    return val"],"metadata":{"id":"pbXs2WpgGTMx","executionInfo":{"status":"ok","timestamp":1692873524353,"user_tz":-600,"elapsed":751,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwIXQlpH-K2d"},"source":["## Checking for CUDA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1692616515511,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"},"user_tz":-600},"id":"zhTfwUiUknDz","outputId":"43c2581a-af6e-473b-edb5-d62f62343f2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is available. You can use the GPU.\n"]}],"source":["if torch.cuda.is_available():\n","    print(\"CUDA is available. You can use the GPU.\")\n","else:\n","    print(\"CUDA is not available. You can only use the CPU.\")"]},{"cell_type":"markdown","metadata":{"id":"1jDttZLv-OPm"},"source":["## Preprocess Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"YP0-K4Mjk2i5","executionInfo":{"status":"ok","timestamp":1692870930000,"user_tz":-600,"elapsed":881,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"}}},"outputs":[],"source":["data_dir = '/content/drive/MyDrive/Machine-Learning-Biomedicine/PankVision-3D/dataset/dataset-007/Data_Train_Test/'"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1070331,"status":"ok","timestamp":1692872001767,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"},"user_tz":-600},"id":"45Ab0DFFI4p_","outputId":"60c48e93-58d7-457b-bda6-cdda124305aa"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n","  warn_deprecated(argname, msg, warning_category)\n","/usr/local/lib/python3.10/dist-packages/monai/utils/deprecate_utils.py:111: FutureWarning: <class 'monai.transforms.utility.dictionary.AddChanneld'>: Class `AddChanneld` has been deprecated since version 0.8. It will be removed in version 1.3. please use MetaTensor data type and monai.transforms.EnsureChannelFirstd instead with `channel_dim='no_channel'`.\n","  warn_deprecated(obj, msg, warning_category)\n","/usr/local/lib/python3.10/dist-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.transforms.utility.dictionary EnsureChannelFirstd.__init__:meta_keys: Argument `meta_keys` has been deprecated since version 0.9. not needed if image is type `MetaTensor`.\n","  warn_deprecated(argname, msg, warning_category)\n","Loading dataset: 100%|██████████| 222/222 [12:10<00:00,  3.29s/it]\n","Loading dataset: 100%|██████████| 56/56 [05:34<00:00,  5.98s/it]\n"]}],"source":["data_in = prepare(data_dir, cache=True)"]},{"cell_type":"markdown","metadata":{"id":"36FPHWIw1GMS"},"source":["# Training"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"RtL9BAWlGN6z","executionInfo":{"status":"ok","timestamp":1692872548423,"user_tz":-600,"elapsed":1208,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"}}},"outputs":[],"source":["model_dir = '/content/drive/MyDrive/Machine-Learning-Biomedicine/PankVision-3D/results/dataset-007/v7'"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":827946,"status":"error","timestamp":1692876047688,"user":{"displayName":"Richard Ji","userId":"09391666119164262630"},"user_tz":-600},"id":"5ROxEVxEfMCR","outputId":"4ffe3474-c893-4197-cb45-b5cc93817b3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------\n","epoch 1/150\n","1/222, Train_loss: 0.7491\n","Train_dice: 0.3845\n","2/222, Train_loss: 0.7434\n","Train_dice: 0.3900\n","3/222, Train_loss: 0.7373\n","Train_dice: 0.3913\n","4/222, Train_loss: 0.7418\n","Train_dice: 0.3869\n","5/222, Train_loss: 0.7346\n","Train_dice: 0.3961\n","6/222, Train_loss: 0.7266\n","Train_dice: 0.3963\n","7/222, Train_loss: 0.7288\n","Train_dice: 0.3925\n","8/222, Train_loss: 0.7227\n","Train_dice: 0.3949\n","9/222, Train_loss: 0.7208\n","Train_dice: 0.3937\n","10/222, Train_loss: 0.7169\n","Train_dice: 0.3939\n","11/222, Train_loss: 0.7130\n","Train_dice: 0.3959\n","12/222, Train_loss: 0.7117\n","Train_dice: 0.3990\n","13/222, Train_loss: 0.7041\n","Train_dice: 0.4041\n","14/222, Train_loss: 0.7038\n","Train_dice: 0.4049\n","15/222, Train_loss: 0.7027\n","Train_dice: 0.3983\n","16/222, Train_loss: 0.7079\n","Train_dice: 0.3966\n","17/222, Train_loss: 0.7022\n","Train_dice: 0.3976\n","18/222, Train_loss: 0.7062\n","Train_dice: 0.3969\n","19/222, Train_loss: 0.7032\n","Train_dice: 0.3937\n","20/222, Train_loss: 0.7024\n","Train_dice: 0.3927\n","21/222, Train_loss: 0.7013\n","Train_dice: 0.3943\n","22/222, Train_loss: 0.6965\n","Train_dice: 0.4055\n","23/222, Train_loss: 0.6992\n","Train_dice: 0.3980\n","24/222, Train_loss: 0.7010\n","Train_dice: 0.3948\n","25/222, Train_loss: 0.7009\n","Train_dice: 0.3874\n","26/222, Train_loss: 0.6910\n","Train_dice: 0.4057\n","27/222, Train_loss: 0.6919\n","Train_dice: 0.4036\n","28/222, Train_loss: 0.7010\n","Train_dice: 0.3889\n","29/222, Train_loss: 0.6907\n","Train_dice: 0.4042\n","30/222, Train_loss: 0.6973\n","Train_dice: 0.3934\n","31/222, Train_loss: 0.6931\n","Train_dice: 0.3992\n","32/222, Train_loss: 0.6920\n","Train_dice: 0.3981\n","33/222, Train_loss: 0.6885\n","Train_dice: 0.4023\n","34/222, Train_loss: 0.6853\n","Train_dice: 0.4074\n","35/222, Train_loss: 0.6863\n","Train_dice: 0.4040\n","36/222, Train_loss: 0.6848\n","Train_dice: 0.4087\n","37/222, Train_loss: 0.6823\n","Train_dice: 0.4110\n","38/222, Train_loss: 0.6883\n","Train_dice: 0.4020\n","39/222, Train_loss: 0.6811\n","Train_dice: 0.4127\n","40/222, Train_loss: 0.6844\n","Train_dice: 0.4080\n","41/222, Train_loss: 0.6929\n","Train_dice: 0.3932\n","42/222, Train_loss: 0.6741\n","Train_dice: 0.4257\n","43/222, Train_loss: 0.6791\n","Train_dice: 0.4142\n","44/222, Train_loss: 0.6906\n","Train_dice: 0.3924\n","45/222, Train_loss: 0.6876\n","Train_dice: 0.3985\n","46/222, Train_loss: 0.6809\n","Train_dice: 0.4094\n","47/222, Train_loss: 0.6888\n","Train_dice: 0.3966\n","48/222, Train_loss: 0.6807\n","Train_dice: 0.4092\n","49/222, Train_loss: 0.6811\n","Train_dice: 0.4080\n","50/222, Train_loss: 0.6826\n","Train_dice: 0.4021\n","51/222, Train_loss: 0.6904\n","Train_dice: 0.3927\n","52/222, Train_loss: 0.6794\n","Train_dice: 0.4071\n","53/222, Train_loss: 0.6760\n","Train_dice: 0.4149\n","54/222, Train_loss: 0.6748\n","Train_dice: 0.4138\n","55/222, Train_loss: 0.6741\n","Train_dice: 0.4162\n","56/222, Train_loss: 0.6887\n","Train_dice: 0.3926\n","57/222, Train_loss: 0.6763\n","Train_dice: 0.4117\n","58/222, Train_loss: 0.6749\n","Train_dice: 0.4137\n","59/222, Train_loss: 0.6875\n","Train_dice: 0.3932\n","60/222, Train_loss: 0.6767\n","Train_dice: 0.4107\n","61/222, Train_loss: 0.6730\n","Train_dice: 0.4161\n","62/222, Train_loss: 0.6735\n","Train_dice: 0.4136\n","63/222, Train_loss: 0.6853\n","Train_dice: 0.3943\n","64/222, Train_loss: 0.6756\n","Train_dice: 0.4105\n","65/222, Train_loss: 0.6746\n","Train_dice: 0.4134\n","66/222, Train_loss: 0.6737\n","Train_dice: 0.4137\n","67/222, Train_loss: 0.6728\n","Train_dice: 0.4156\n","68/222, Train_loss: 0.6765\n","Train_dice: 0.4091\n","69/222, Train_loss: 0.6751\n","Train_dice: 0.4106\n","70/222, Train_loss: 0.6722\n","Train_dice: 0.4163\n","71/222, Train_loss: 0.6762\n","Train_dice: 0.4088\n","72/222, Train_loss: 0.6712\n","Train_dice: 0.4178\n","73/222, Train_loss: 0.6758\n","Train_dice: 0.4086\n","74/222, Train_loss: 0.6749\n","Train_dice: 0.4081\n","75/222, Train_loss: 0.6737\n","Train_dice: 0.4100\n","76/222, Train_loss: 0.6768\n","Train_dice: 0.4045\n","77/222, Train_loss: 0.6745\n","Train_dice: 0.4100\n","78/222, Train_loss: 0.6719\n","Train_dice: 0.4157\n","79/222, Train_loss: 0.6749\n","Train_dice: 0.4078\n","80/222, Train_loss: 0.6749\n","Train_dice: 0.4087\n","81/222, Train_loss: 0.6741\n","Train_dice: 0.4088\n","82/222, Train_loss: 0.6736\n","Train_dice: 0.4088\n","83/222, Train_loss: 0.6707\n","Train_dice: 0.4143\n","84/222, Train_loss: 0.6730\n","Train_dice: 0.4088\n","85/222, Train_loss: 0.6710\n","Train_dice: 0.4147\n","86/222, Train_loss: 0.6755\n","Train_dice: 0.4040\n","87/222, Train_loss: 0.6751\n","Train_dice: 0.4074\n","88/222, Train_loss: 0.6696\n","Train_dice: 0.4177\n","89/222, Train_loss: 0.6731\n","Train_dice: 0.4097\n","90/222, Train_loss: 0.6748\n","Train_dice: 0.4062\n","91/222, Train_loss: 0.6733\n","Train_dice: 0.4078\n","92/222, Train_loss: 0.6678\n","Train_dice: 0.4187\n","93/222, Train_loss: 0.6719\n","Train_dice: 0.4110\n","94/222, Train_loss: 0.6746\n","Train_dice: 0.4079\n","95/222, Train_loss: 0.6748\n","Train_dice: 0.4061\n","96/222, Train_loss: 0.6694\n","Train_dice: 0.4147\n","97/222, Train_loss: 0.6712\n","Train_dice: 0.4118\n","98/222, Train_loss: 0.6746\n","Train_dice: 0.4040\n","99/222, Train_loss: 0.6719\n","Train_dice: 0.4092\n","100/222, Train_loss: 0.6720\n","Train_dice: 0.4081\n","101/222, Train_loss: 0.6705\n","Train_dice: 0.4118\n","102/222, Train_loss: 0.6689\n","Train_dice: 0.4151\n","103/222, Train_loss: 0.6649\n","Train_dice: 0.4210\n","104/222, Train_loss: 0.6771\n","Train_dice: 0.3988\n","105/222, Train_loss: 0.6748\n","Train_dice: 0.4031\n","106/222, Train_loss: 0.6745\n","Train_dice: 0.4043\n","107/222, Train_loss: 0.6702\n","Train_dice: 0.4102\n","108/222, Train_loss: 0.6705\n","Train_dice: 0.4109\n","109/222, Train_loss: 0.6705\n","Train_dice: 0.4093\n","110/222, Train_loss: 0.6714\n","Train_dice: 0.4085\n","111/222, Train_loss: 0.6707\n","Train_dice: 0.4102\n","112/222, Train_loss: 0.6685\n","Train_dice: 0.4141\n","113/222, Train_loss: 0.6685\n","Train_dice: 0.4134\n","114/222, Train_loss: 0.6716\n","Train_dice: 0.4070\n","115/222, Train_loss: 0.6696\n","Train_dice: 0.4128\n","116/222, Train_loss: 0.6690\n","Train_dice: 0.4126\n","117/222, Train_loss: 0.6659\n","Train_dice: 0.4168\n","118/222, Train_loss: 0.6699\n","Train_dice: 0.4100\n","119/222, Train_loss: 0.6690\n","Train_dice: 0.4108\n","120/222, Train_loss: 0.6653\n","Train_dice: 0.4186\n","121/222, Train_loss: 0.6675\n","Train_dice: 0.4136\n","122/222, Train_loss: 0.6627\n","Train_dice: 0.4238\n","123/222, Train_loss: 0.6676\n","Train_dice: 0.4130\n","124/222, Train_loss: 0.6736\n","Train_dice: 0.4028\n","125/222, Train_loss: 0.6712\n","Train_dice: 0.4078\n","126/222, Train_loss: 0.6673\n","Train_dice: 0.4137\n","127/222, Train_loss: 0.6692\n","Train_dice: 0.4100\n","128/222, Train_loss: 0.6602\n","Train_dice: 0.4257\n","129/222, Train_loss: 0.6632\n","Train_dice: 0.4208\n","130/222, Train_loss: 0.6692\n","Train_dice: 0.4095\n","131/222, Train_loss: 0.6651\n","Train_dice: 0.4174\n","132/222, Train_loss: 0.6643\n","Train_dice: 0.4171\n","133/222, Train_loss: 0.6670\n","Train_dice: 0.4131\n","134/222, Train_loss: 0.6660\n","Train_dice: 0.4139\n","135/222, Train_loss: 0.6611\n","Train_dice: 0.4220\n","136/222, Train_loss: 0.6640\n","Train_dice: 0.4185\n","137/222, Train_loss: 0.6667\n","Train_dice: 0.4125\n","138/222, Train_loss: 0.6639\n","Train_dice: 0.4174\n","139/222, Train_loss: 0.6614\n","Train_dice: 0.4218\n","140/222, Train_loss: 0.6633\n","Train_dice: 0.4173\n","141/222, Train_loss: 0.6632\n","Train_dice: 0.4174\n","142/222, Train_loss: 0.6630\n","Train_dice: 0.4184\n","143/222, Train_loss: 0.6619\n","Train_dice: 0.4202\n","144/222, Train_loss: 0.6615\n","Train_dice: 0.4194\n","145/222, Train_loss: 0.6623\n","Train_dice: 0.4191\n","146/222, Train_loss: 0.6643\n","Train_dice: 0.4151\n","147/222, Train_loss: 0.6594\n","Train_dice: 0.4256\n","148/222, Train_loss: 0.6602\n","Train_dice: 0.4225\n","149/222, Train_loss: 0.6666\n","Train_dice: 0.4110\n","150/222, Train_loss: 0.6629\n","Train_dice: 0.4170\n","151/222, Train_loss: 0.6652\n","Train_dice: 0.4139\n","152/222, Train_loss: 0.6628\n","Train_dice: 0.4182\n","153/222, Train_loss: 0.6600\n","Train_dice: 0.4223\n","154/222, Train_loss: 0.6645\n","Train_dice: 0.4137\n","155/222, Train_loss: 0.6600\n","Train_dice: 0.4214\n","156/222, Train_loss: 0.6627\n","Train_dice: 0.4153\n","157/222, Train_loss: 0.6586\n","Train_dice: 0.4235\n","158/222, Train_loss: 0.6587\n","Train_dice: 0.4224\n","159/222, Train_loss: 0.6610\n","Train_dice: 0.4177\n","160/222, Train_loss: 0.6570\n","Train_dice: 0.4252\n","161/222, Train_loss: 0.6642\n","Train_dice: 0.4116\n","162/222, Train_loss: 0.6589\n","Train_dice: 0.4208\n","163/222, Train_loss: 0.6600\n","Train_dice: 0.4190\n","164/222, Train_loss: 0.6557\n","Train_dice: 0.4258\n","165/222, Train_loss: 0.6591\n","Train_dice: 0.4194\n","166/222, Train_loss: 0.6550\n","Train_dice: 0.4279\n","167/222, Train_loss: 0.6558\n","Train_dice: 0.4250\n","168/222, Train_loss: 0.6572\n","Train_dice: 0.4225\n","169/222, Train_loss: 0.6558\n","Train_dice: 0.4244\n","170/222, Train_loss: 0.6500\n","Train_dice: 0.4338\n","171/222, Train_loss: 0.6589\n","Train_dice: 0.4187\n","172/222, Train_loss: 0.6564\n","Train_dice: 0.4227\n","173/222, Train_loss: 0.6588\n","Train_dice: 0.4183\n","174/222, Train_loss: 0.6542\n","Train_dice: 0.4258\n","175/222, Train_loss: 0.6548\n","Train_dice: 0.4240\n","176/222, Train_loss: 0.6521\n","Train_dice: 0.4287\n","177/222, Train_loss: 0.6536\n","Train_dice: 0.4252\n","178/222, Train_loss: 0.6581\n","Train_dice: 0.4237\n","179/222, Train_loss: 0.6565\n","Train_dice: 0.4222\n","180/222, Train_loss: 0.6601\n","Train_dice: 0.4154\n","181/222, Train_loss: 0.6532\n","Train_dice: 0.4261\n","182/222, Train_loss: 0.6478\n","Train_dice: 0.4361\n","183/222, Train_loss: 0.6591\n","Train_dice: 0.4167\n","184/222, Train_loss: 0.6495\n","Train_dice: 0.4304\n","185/222, Train_loss: 0.6464\n","Train_dice: 0.4355\n","186/222, Train_loss: 0.6471\n","Train_dice: 0.4340\n","187/222, Train_loss: 0.6519\n","Train_dice: 0.4278\n","188/222, Train_loss: 0.6448\n","Train_dice: 0.4372\n","189/222, Train_loss: 0.6489\n","Train_dice: 0.4296\n","190/222, Train_loss: 0.6481\n","Train_dice: 0.4309\n","191/222, Train_loss: 0.6565\n","Train_dice: 0.4178\n","192/222, Train_loss: 0.6558\n","Train_dice: 0.4185\n","193/222, Train_loss: 0.6559\n","Train_dice: 0.4177\n","194/222, Train_loss: 0.6428\n","Train_dice: 0.4386\n","195/222, Train_loss: 0.6544\n","Train_dice: 0.4192\n","196/222, Train_loss: 0.6430\n","Train_dice: 0.4374\n","197/222, Train_loss: 0.6504\n","Train_dice: 0.4241\n","198/222, Train_loss: 0.6363\n","Train_dice: 0.4479\n","199/222, Train_loss: 0.6437\n","Train_dice: 0.4336\n","200/222, Train_loss: 0.6459\n","Train_dice: 0.4304\n","201/222, Train_loss: 0.6446\n","Train_dice: 0.4310\n","202/222, Train_loss: 0.6506\n","Train_dice: 0.4219\n","203/222, Train_loss: 0.6496\n","Train_dice: 0.4230\n","204/222, Train_loss: 0.6428\n","Train_dice: 0.4339\n","205/222, Train_loss: 0.6420\n","Train_dice: 0.4341\n","206/222, Train_loss: 0.6355\n","Train_dice: 0.4434\n","207/222, Train_loss: 0.6471\n","Train_dice: 0.4251\n","208/222, Train_loss: 0.6465\n","Train_dice: 0.4256\n","209/222, Train_loss: 0.6472\n","Train_dice: 0.4249\n","210/222, Train_loss: 0.6453\n","Train_dice: 0.4263\n","211/222, Train_loss: 0.6452\n","Train_dice: 0.4260\n","212/222, Train_loss: 0.6449\n","Train_dice: 0.4261\n","213/222, Train_loss: 0.6351\n","Train_dice: 0.4416\n","214/222, Train_loss: 0.6380\n","Train_dice: 0.4380\n","215/222, Train_loss: 0.6294\n","Train_dice: 0.4486\n","216/222, Train_loss: 0.6329\n","Train_dice: 0.4425\n","217/222, Train_loss: 0.6432\n","Train_dice: 0.4270\n","218/222, Train_loss: 0.6354\n","Train_dice: 0.4382\n","219/222, Train_loss: 0.6298\n","Train_dice: 0.4454\n","220/222, Train_loss: 0.6393\n","Train_dice: 0.4316\n","221/222, Train_loss: 0.6411\n","Train_dice: 0.4285\n","222/222, Train_loss: 0.6368\n","Train_dice: 0.4350\n","--------------------\n","Epoch_loss: 0.6710\n","Epoch_metric: 0.4141\n","test_loss_epoch: 0.6313\n","test_dice_epoch: 0.4435\n","Confusion Matrix:\n","[[47744757 10618777]\n"," [  207228   149494]]\n","test_loss_epoch: 0.0113\n","test_dice_epoch: 0.0079\n","current epoch: 1 current mean dice: 0.4387\n","best mean dice: 0.0079 at epoch: 1\n","----------\n","epoch 2/150\n","1/222, Train_loss: 0.6289\n","Train_dice: 0.4454\n","2/222, Train_loss: 0.6242\n","Train_dice: 0.4525\n","3/222, Train_loss: 0.6216\n","Train_dice: 0.4546\n","4/222, Train_loss: 0.6261\n","Train_dice: 0.4478\n","5/222, Train_loss: 0.6253\n","Train_dice: 0.4531\n","6/222, Train_loss: 0.6208\n","Train_dice: 0.4556\n","7/222, Train_loss: 0.6249\n","Train_dice: 0.4489\n","8/222, Train_loss: 0.6221\n","Train_dice: 0.4524\n","9/222, Train_loss: 0.6222\n","Train_dice: 0.4509\n","10/222, Train_loss: 0.6230\n","Train_dice: 0.4497\n","11/222, Train_loss: 0.6236\n","Train_dice: 0.4490\n","12/222, Train_loss: 0.6192\n","Train_dice: 0.4548\n","13/222, Train_loss: 0.6185\n","Train_dice: 0.4569\n","14/222, Train_loss: 0.6117\n","Train_dice: 0.4644\n","15/222, Train_loss: 0.6205\n","Train_dice: 0.4517\n","16/222, Train_loss: 0.6188\n","Train_dice: 0.4525\n","17/222, Train_loss: 0.6213\n","Train_dice: 0.4500\n","18/222, Train_loss: 0.6236\n","Train_dice: 0.4489\n","19/222, Train_loss: 0.6266\n","Train_dice: 0.4419\n","20/222, Train_loss: 0.6245\n","Train_dice: 0.4437\n","21/222, Train_loss: 0.6239\n","Train_dice: 0.4442\n","22/222, Train_loss: 0.6148\n","Train_dice: 0.4578\n","23/222, Train_loss: 0.6175\n","Train_dice: 0.4522\n","24/222, Train_loss: 0.6188\n","Train_dice: 0.4492\n","25/222, Train_loss: 0.6296\n","Train_dice: 0.4355\n","26/222, Train_loss: 0.6102\n","Train_dice: 0.4610\n","27/222, Train_loss: 0.6107\n","Train_dice: 0.4594\n","28/222, Train_loss: 0.6268\n","Train_dice: 0.4377\n","29/222, Train_loss: 0.6125\n","Train_dice: 0.4566\n","30/222, Train_loss: 0.6245\n","Train_dice: 0.4417\n","31/222, Train_loss: 0.6154\n","Train_dice: 0.4515\n","32/222, Train_loss: 0.6215\n","Train_dice: 0.4446\n","33/222, Train_loss: 0.6128\n","Train_dice: 0.4550\n","34/222, Train_loss: 0.6053\n","Train_dice: 0.4634\n","35/222, Train_loss: 0.6123\n","Train_dice: 0.4545\n","36/222, Train_loss: 0.6017\n","Train_dice: 0.4675\n","37/222, Train_loss: 0.6026\n","Train_dice: 0.4670\n","38/222, Train_loss: 0.6119\n","Train_dice: 0.4531\n","39/222, Train_loss: 0.6021\n","Train_dice: 0.4665\n","40/222, Train_loss: 0.6044\n","Train_dice: 0.4619\n","41/222, Train_loss: 0.6194\n","Train_dice: 0.4425\n","42/222, Train_loss: 0.5920\n","Train_dice: 0.4810\n","43/222, Train_loss: 0.6008\n","Train_dice: 0.4662\n","44/222, Train_loss: 0.6200\n","Train_dice: 0.4402\n","45/222, Train_loss: 0.6157\n","Train_dice: 0.4472\n","46/222, Train_loss: 0.6067\n","Train_dice: 0.4579\n","47/222, Train_loss: 0.6159\n","Train_dice: 0.4446\n","48/222, Train_loss: 0.6023\n","Train_dice: 0.4623\n","49/222, Train_loss: 0.6057\n","Train_dice: 0.4582\n","50/222, Train_loss: 0.6103\n","Train_dice: 0.4514\n","51/222, Train_loss: 0.6173\n","Train_dice: 0.4410\n","52/222, Train_loss: 0.6073\n","Train_dice: 0.4551\n","53/222, Train_loss: 0.5976\n","Train_dice: 0.4664\n","54/222, Train_loss: 0.6019\n","Train_dice: 0.4627\n","55/222, Train_loss: 0.5988\n","Train_dice: 0.4659\n","56/222, Train_loss: 0.6156\n","Train_dice: 0.4416\n","57/222, Train_loss: 0.5983\n","Train_dice: 0.4651\n","58/222, Train_loss: 0.5975\n","Train_dice: 0.4656\n","59/222, Train_loss: 0.6146\n","Train_dice: 0.4419\n","60/222, Train_loss: 0.5982\n","Train_dice: 0.4630\n","61/222, Train_loss: 0.5961\n","Train_dice: 0.4665\n","62/222, Train_loss: 0.5980\n","Train_dice: 0.4677\n","63/222, Train_loss: 0.6127\n","Train_dice: 0.4427\n","64/222, Train_loss: 0.5966\n","Train_dice: 0.4649\n","65/222, Train_loss: 0.5934\n","Train_dice: 0.4675\n","66/222, Train_loss: 0.5941\n","Train_dice: 0.4666\n","67/222, Train_loss: 0.5928\n","Train_dice: 0.4679\n","68/222, Train_loss: 0.5966\n","Train_dice: 0.4630\n","69/222, Train_loss: 0.5969\n","Train_dice: 0.4621\n","70/222, Train_loss: 0.5901\n","Train_dice: 0.4707\n","71/222, Train_loss: 0.5982\n","Train_dice: 0.4598\n","72/222, Train_loss: 0.5899\n","Train_dice: 0.4705\n","73/222, Train_loss: 0.5974\n","Train_dice: 0.4606\n","74/222, Train_loss: 0.6009\n","Train_dice: 0.4562\n","75/222, Train_loss: 0.5974\n","Train_dice: 0.4602\n","76/222, Train_loss: 0.5998\n","Train_dice: 0.4558\n","77/222, Train_loss: 0.5965\n","Train_dice: 0.4599\n","78/222, Train_loss: 0.5939\n","Train_dice: 0.4656\n","79/222, Train_loss: 0.5958\n","Train_dice: 0.4601\n","80/222, Train_loss: 0.5953\n","Train_dice: 0.4605\n","81/222, Train_loss: 0.5958\n","Train_dice: 0.4596\n","82/222, Train_loss: 0.5950\n","Train_dice: 0.4605\n","83/222, Train_loss: 0.5889\n","Train_dice: 0.4681\n","84/222, Train_loss: 0.5937\n","Train_dice: 0.4616\n","85/222, Train_loss: 0.5890\n","Train_dice: 0.4669\n","86/222, Train_loss: 0.5980\n","Train_dice: 0.4558\n","87/222, Train_loss: 0.5967\n","Train_dice: 0.4578\n","88/222, Train_loss: 0.5847\n","Train_dice: 0.4730\n","89/222, Train_loss: 0.5913\n","Train_dice: 0.4631\n","90/222, Train_loss: 0.5949\n","Train_dice: 0.4582\n","91/222, Train_loss: 0.5929\n","Train_dice: 0.4605\n","92/222, Train_loss: 0.5816\n","Train_dice: 0.4746\n","93/222, Train_loss: 0.5889\n","Train_dice: 0.4652\n","94/222, Train_loss: 0.5906\n","Train_dice: 0.4626\n","95/222, Train_loss: 0.5932\n","Train_dice: 0.4591\n","96/222, Train_loss: 0.5845\n","Train_dice: 0.4695\n","97/222, Train_loss: 0.5877\n","Train_dice: 0.4649\n","98/222, Train_loss: 0.5969\n","Train_dice: 0.4541\n","99/222, Train_loss: 0.5925\n","Train_dice: 0.4603\n","100/222, Train_loss: 0.5925\n","Train_dice: 0.4597\n","101/222, Train_loss: 0.5859\n","Train_dice: 0.4667\n","102/222, Train_loss: 0.5845\n","Train_dice: 0.4682\n","103/222, Train_loss: 0.5796\n","Train_dice: 0.4750\n","104/222, Train_loss: 0.5987\n","Train_dice: 0.4495\n","105/222, Train_loss: 0.5934\n","Train_dice: 0.4558\n","106/222, Train_loss: 0.5916\n","Train_dice: 0.4591\n","107/222, Train_loss: 0.5880\n","Train_dice: 0.4625\n","108/222, Train_loss: 0.5859\n","Train_dice: 0.4643\n","109/222, Train_loss: 0.5859\n","Train_dice: 0.4640\n","110/222, Train_loss: 0.5890\n","Train_dice: 0.4604\n","111/222, Train_loss: 0.5878\n","Train_dice: 0.4618\n","112/222, Train_loss: 0.5805\n","Train_dice: 0.4712\n","113/222, Train_loss: 0.5817\n","Train_dice: 0.4696\n","114/222, Train_loss: 0.5877\n","Train_dice: 0.4608\n","115/222, Train_loss: 0.5835\n","Train_dice: 0.4672\n","116/222, Train_loss: 0.5814\n","Train_dice: 0.4685\n","117/222, Train_loss: 0.5790\n","Train_dice: 0.4716\n","118/222, Train_loss: 0.5855\n","Train_dice: 0.4625\n","119/222, Train_loss: 0.5851\n","Train_dice: 0.4641\n","120/222, Train_loss: 0.5750\n","Train_dice: 0.4754\n","121/222, Train_loss: 0.5790\n","Train_dice: 0.4709\n","122/222, Train_loss: 0.5684\n","Train_dice: 0.4832\n","123/222, Train_loss: 0.5807\n","Train_dice: 0.4670\n","124/222, Train_loss: 0.5911\n","Train_dice: 0.4538\n","125/222, Train_loss: 0.5882\n","Train_dice: 0.4591\n","126/222, Train_loss: 0.5797\n","Train_dice: 0.4689\n","127/222, Train_loss: 0.5824\n","Train_dice: 0.4639\n","128/222, Train_loss: 0.5700\n","Train_dice: 0.4805\n","129/222, Train_loss: 0.5706\n","Train_dice: 0.4784\n","130/222, Train_loss: 0.5850\n","Train_dice: 0.4625\n","131/222, Train_loss: 0.5748\n","Train_dice: 0.4731\n","132/222, Train_loss: 0.5754\n","Train_dice: 0.4728\n","133/222, Train_loss: 0.5782\n","Train_dice: 0.4678\n","134/222, Train_loss: 0.5794\n","Train_dice: 0.4672\n","135/222, Train_loss: 0.5712\n","Train_dice: 0.4784\n","136/222, Train_loss: 0.5726\n","Train_dice: 0.4738\n","137/222, Train_loss: 0.5786\n","Train_dice: 0.4664\n","138/222, Train_loss: 0.5754\n","Train_dice: 0.4723\n","139/222, Train_loss: 0.5693\n","Train_dice: 0.4781\n","140/222, Train_loss: 0.5739\n","Train_dice: 0.4726\n","141/222, Train_loss: 0.5733\n","Train_dice: 0.4730\n","142/222, Train_loss: 0.5743\n","Train_dice: 0.4715\n","143/222, Train_loss: 0.5708\n","Train_dice: 0.4756\n","144/222, Train_loss: 0.5717\n","Train_dice: 0.4744\n","145/222, Train_loss: 0.5717\n","Train_dice: 0.4730\n","146/222, Train_loss: 0.5765\n","Train_dice: 0.4679\n","147/222, Train_loss: 0.5688\n","Train_dice: 0.4800\n","148/222, Train_loss: 0.5673\n","Train_dice: 0.4796\n","149/222, Train_loss: 0.5805\n","Train_dice: 0.4621\n","150/222, Train_loss: 0.5735\n","Train_dice: 0.4701\n","151/222, Train_loss: 0.5788\n","Train_dice: 0.4645\n","152/222, Train_loss: 0.5680\n","Train_dice: 0.4775\n","153/222, Train_loss: 0.5696\n","Train_dice: 0.4744\n","154/222, Train_loss: 0.5777\n","Train_dice: 0.4646\n","155/222, Train_loss: 0.5702\n","Train_dice: 0.4740\n","156/222, Train_loss: 0.5760\n","Train_dice: 0.4673\n","157/222, Train_loss: 0.5669\n","Train_dice: 0.4769\n","158/222, Train_loss: 0.5692\n","Train_dice: 0.4754\n","159/222, Train_loss: 0.5748\n","Train_dice: 0.4677\n","160/222, Train_loss: 0.5653\n","Train_dice: 0.4786\n","161/222, Train_loss: 0.5791\n","Train_dice: 0.4623\n","162/222, Train_loss: 0.5697\n","Train_dice: 0.4730\n","163/222, Train_loss: 0.5716\n","Train_dice: 0.4702\n","164/222, Train_loss: 0.5647\n","Train_dice: 0.4791\n","165/222, Train_loss: 0.5719\n","Train_dice: 0.4693\n","166/222, Train_loss: 0.5590\n","Train_dice: 0.4852\n","167/222, Train_loss: 0.5654\n","Train_dice: 0.4773\n","168/222, Train_loss: 0.5694\n","Train_dice: 0.4732\n","169/222, Train_loss: 0.5664\n","Train_dice: 0.4756\n","170/222, Train_loss: 0.5579\n","Train_dice: 0.4866\n","171/222, Train_loss: 0.5736\n","Train_dice: 0.4663\n","172/222, Train_loss: 0.5685\n","Train_dice: 0.4722\n","173/222, Train_loss: 0.5746\n","Train_dice: 0.4646\n","174/222, Train_loss: 0.5658\n","Train_dice: 0.4764\n","175/222, Train_loss: 0.5688\n","Train_dice: 0.4717\n","176/222, Train_loss: 0.5625\n","Train_dice: 0.4786\n","177/222, Train_loss: 0.5668\n","Train_dice: 0.4730\n","178/222, Train_loss: 0.5651\n","Train_dice: 0.4768\n","179/222, Train_loss: 0.5749\n","Train_dice: 0.4662\n","180/222, Train_loss: 0.5782\n","Train_dice: 0.4598\n","181/222, Train_loss: 0.5695\n","Train_dice: 0.4712\n","182/222, Train_loss: 0.5540\n","Train_dice: 0.4903\n","183/222, Train_loss: 0.5770\n","Train_dice: 0.4606\n","184/222, Train_loss: 0.5605\n","Train_dice: 0.4798\n","185/222, Train_loss: 0.5533\n","Train_dice: 0.4888\n","186/222, Train_loss: 0.5567\n","Train_dice: 0.4848\n","187/222, Train_loss: 0.5614\n","Train_dice: 0.4785\n","188/222, Train_loss: 0.5554\n","Train_dice: 0.4877\n","189/222, Train_loss: 0.5604\n","Train_dice: 0.4784\n","190/222, Train_loss: 0.5600\n","Train_dice: 0.4801\n","191/222, Train_loss: 0.5777\n","Train_dice: 0.4579\n","192/222, Train_loss: 0.5804\n","Train_dice: 0.4568\n","193/222, Train_loss: 0.5769\n","Train_dice: 0.4587\n","194/222, Train_loss: 0.5505\n","Train_dice: 0.4910\n","195/222, Train_loss: 0.5779\n","Train_dice: 0.4583\n","196/222, Train_loss: 0.5537\n","Train_dice: 0.4862\n","197/222, Train_loss: 0.5715\n","Train_dice: 0.4651\n","198/222, Train_loss: 0.5455\n","Train_dice: 0.4988\n","199/222, Train_loss: 0.5623\n","Train_dice: 0.4764\n","200/222, Train_loss: 0.5633\n","Train_dice: 0.4739\n","201/222, Train_loss: 0.5625\n","Train_dice: 0.4744\n","202/222, Train_loss: 0.5753\n","Train_dice: 0.4592\n","203/222, Train_loss: 0.5743\n","Train_dice: 0.4601\n","204/222, Train_loss: 0.5587\n","Train_dice: 0.4795\n","205/222, Train_loss: 0.5616\n","Train_dice: 0.4751\n","206/222, Train_loss: 0.5504\n","Train_dice: 0.4896\n","207/222, Train_loss: 0.5733\n","Train_dice: 0.4603\n","208/222, Train_loss: 0.5728\n","Train_dice: 0.4609\n","209/222, Train_loss: 0.5769\n","Train_dice: 0.4592\n","210/222, Train_loss: 0.5724\n","Train_dice: 0.4609\n","211/222, Train_loss: 0.5724\n","Train_dice: 0.4608\n","212/222, Train_loss: 0.5720\n","Train_dice: 0.4610\n","213/222, Train_loss: 0.5555\n","Train_dice: 0.4829\n","214/222, Train_loss: 0.5685\n","Train_dice: 0.4717\n","215/222, Train_loss: 0.5436\n","Train_dice: 0.4954\n","216/222, Train_loss: 0.5516\n","Train_dice: 0.4852\n","217/222, Train_loss: 0.5718\n","Train_dice: 0.4617\n","218/222, Train_loss: 0.5573\n","Train_dice: 0.4782\n","219/222, Train_loss: 0.5523\n","Train_dice: 0.4846\n","220/222, Train_loss: 0.5661\n","Train_dice: 0.4679\n","221/222, Train_loss: 0.5708\n","Train_dice: 0.4623\n","222/222, Train_loss: 0.5685\n","Train_dice: 0.4675\n","--------------------\n","Epoch_loss: 0.5874\n","Epoch_metric: 0.4650\n","test_loss_epoch: 0.5495\n","test_dice_epoch: 0.4871\n","Confusion Matrix:\n","[[57049921  1313613]\n"," [  117043   239679]]\n","test_loss_epoch: 0.0098\n","test_dice_epoch: 0.0087\n","current epoch: 2 current mean dice: 0.4786\n","best mean dice: 0.0087 at epoch: 2\n","----------\n","epoch 3/150\n","1/222, Train_loss: 0.5483\n","Train_dice: 0.4874\n","2/222, Train_loss: 0.5444\n","Train_dice: 0.4940\n","3/222, Train_loss: 0.5446\n","Train_dice: 0.4933\n","4/222, Train_loss: 0.5481\n","Train_dice: 0.4878\n","5/222, Train_loss: 0.5524\n","Train_dice: 0.4907\n","6/222, Train_loss: 0.5422\n","Train_dice: 0.4967\n","7/222, Train_loss: 0.5496\n","Train_dice: 0.4870\n","8/222, Train_loss: 0.5454\n","Train_dice: 0.4915\n","9/222, Train_loss: 0.5447\n","Train_dice: 0.4907\n","10/222, Train_loss: 0.5466\n","Train_dice: 0.4890\n","11/222, Train_loss: 0.5486\n","Train_dice: 0.4874\n","12/222, Train_loss: 0.5449\n","Train_dice: 0.4926\n","13/222, Train_loss: 0.5385\n","Train_dice: 0.5009\n","14/222, Train_loss: 0.5297\n","Train_dice: 0.5086\n","15/222, Train_loss: 0.5476\n","Train_dice: 0.4888\n","16/222, Train_loss: 0.5467\n","Train_dice: 0.4881\n","17/222, Train_loss: 0.5492\n","Train_dice: 0.4864\n","18/222, Train_loss: 0.5482\n","Train_dice: 0.4894\n","19/222, Train_loss: 0.5576\n","Train_dice: 0.4755\n","20/222, Train_loss: 0.5569\n","Train_dice: 0.4762\n","21/222, Train_loss: 0.5556\n","Train_dice: 0.4773\n","22/222, Train_loss: 0.5464\n","Train_dice: 0.4921\n","23/222, Train_loss: 0.5444\n","Train_dice: 0.4894\n","24/222, Train_loss: 0.5509\n","Train_dice: 0.4819\n","25/222, Train_loss: 0.5691\n","Train_dice: 0.4640\n","26/222, Train_loss: 0.5350\n","Train_dice: 0.5003\n","27/222, Train_loss: 0.5368\n","Train_dice: 0.4978\n","28/222, Train_loss: 0.5629\n","Train_dice: 0.4677\n","29/222, Train_loss: 0.5402\n","Train_dice: 0.4941\n","30/222, Train_loss: 0.5617\n","Train_dice: 0.4719\n","31/222, Train_loss: 0.5441\n","Train_dice: 0.4883\n","32/222, Train_loss: 0.5572\n","Train_dice: 0.4766\n","33/222, Train_loss: 0.5404\n","Train_dice: 0.4935\n","34/222, Train_loss: 0.5337\n","Train_dice: 0.5009\n","35/222, Train_loss: 0.5430\n","Train_dice: 0.4900\n","36/222, Train_loss: 0.5321\n","Train_dice: 0.5036\n","37/222, Train_loss: 0.5314\n","Train_dice: 0.5049\n","38/222, Train_loss: 0.5438\n","Train_dice: 0.4880\n","39/222, Train_loss: 0.5294\n","Train_dice: 0.5052\n","40/222, Train_loss: 0.5355\n","Train_dice: 0.4970\n","41/222, Train_loss: 0.5578\n","Train_dice: 0.4721\n","42/222, Train_loss: 0.5138\n","Train_dice: 0.5258\n","43/222, Train_loss: 0.5299\n","Train_dice: 0.5037\n","44/222, Train_loss: 0.5627\n","Train_dice: 0.4663\n","45/222, Train_loss: 0.5537\n","Train_dice: 0.4780\n","46/222, Train_loss: 0.5387\n","Train_dice: 0.4935\n","47/222, Train_loss: 0.5567\n","Train_dice: 0.4731\n","48/222, Train_loss: 0.5358\n","Train_dice: 0.4963\n","49/222, Train_loss: 0.5422\n","Train_dice: 0.4904\n","50/222, Train_loss: 0.5499\n","Train_dice: 0.4806\n","51/222, Train_loss: 0.5629\n","Train_dice: 0.4654\n","52/222, Train_loss: 0.5419\n","Train_dice: 0.4890\n","53/222, Train_loss: 0.5296\n","Train_dice: 0.5019\n","54/222, Train_loss: 0.5328\n","Train_dice: 0.5005\n","55/222, Train_loss: 0.5305\n","Train_dice: 0.5025\n","56/222, Train_loss: 0.5622\n","Train_dice: 0.4652\n","57/222, Train_loss: 0.5305\n","Train_dice: 0.5019\n","58/222, Train_loss: 0.5313\n","Train_dice: 0.5006\n","59/222, Train_loss: 0.5607\n","Train_dice: 0.4664\n","60/222, Train_loss: 0.5372\n","Train_dice: 0.4940\n","61/222, Train_loss: 0.5281\n","Train_dice: 0.5036\n","62/222, Train_loss: 0.5370\n","Train_dice: 0.5013\n","63/222, Train_loss: 0.5599\n","Train_dice: 0.4664\n","64/222, Train_loss: 0.5319\n","Train_dice: 0.4997\n","65/222, Train_loss: 0.5298\n","Train_dice: 0.5007\n","66/222, Train_loss: 0.5285\n","Train_dice: 0.5017\n","67/222, Train_loss: 0.5309\n","Train_dice: 0.4999\n","68/222, Train_loss: 0.5327\n","Train_dice: 0.4974\n","69/222, Train_loss: 0.5366\n","Train_dice: 0.4932\n","70/222, Train_loss: 0.5269\n","Train_dice: 0.5045\n","71/222, Train_loss: 0.5378\n","Train_dice: 0.4912\n","72/222, Train_loss: 0.5232\n","Train_dice: 0.5071\n","73/222, Train_loss: 0.5380\n","Train_dice: 0.4916\n","74/222, Train_loss: 0.5413\n","Train_dice: 0.4876\n","75/222, Train_loss: 0.5351\n","Train_dice: 0.4937\n","76/222, Train_loss: 0.5453\n","Train_dice: 0.4824\n","77/222, Train_loss: 0.5388\n","Train_dice: 0.4892\n","78/222, Train_loss: 0.5332\n","Train_dice: 0.4984\n","79/222, Train_loss: 0.5388\n","Train_dice: 0.4893\n","80/222, Train_loss: 0.5381\n","Train_dice: 0.4899\n","81/222, Train_loss: 0.5382\n","Train_dice: 0.4891\n","82/222, Train_loss: 0.5391\n","Train_dice: 0.4888\n","83/222, Train_loss: 0.5286\n","Train_dice: 0.4999\n","84/222, Train_loss: 0.5370\n","Train_dice: 0.4908\n","85/222, Train_loss: 0.5275\n","Train_dice: 0.5004\n","86/222, Train_loss: 0.5478\n","Train_dice: 0.4793\n","87/222, Train_loss: 0.5405\n","Train_dice: 0.4870\n","88/222, Train_loss: 0.5166\n","Train_dice: 0.5125\n","89/222, Train_loss: 0.5351\n","Train_dice: 0.4932\n","90/222, Train_loss: 0.5423\n","Train_dice: 0.4845\n","91/222, Train_loss: 0.5358\n","Train_dice: 0.4907\n","92/222, Train_loss: 0.5225\n","Train_dice: 0.5064\n","93/222, Train_loss: 0.5324\n","Train_dice: 0.4951\n","94/222, Train_loss: 0.5464\n","Train_dice: 0.4835\n","95/222, Train_loss: 0.5425\n","Train_dice: 0.4839\n","96/222, Train_loss: 0.5265\n","Train_dice: 0.5004\n","97/222, Train_loss: 0.5329\n","Train_dice: 0.4934\n","98/222, Train_loss: 0.5442\n","Train_dice: 0.4812\n","99/222, Train_loss: 0.5340\n","Train_dice: 0.4932\n","100/222, Train_loss: 0.5383\n","Train_dice: 0.4886\n","101/222, Train_loss: 0.5295\n","Train_dice: 0.4970\n","102/222, Train_loss: 0.5292\n","Train_dice: 0.4974\n","103/222, Train_loss: 0.5208\n","Train_dice: 0.5075\n","104/222, Train_loss: 0.5540\n","Train_dice: 0.4697\n","105/222, Train_loss: 0.5447\n","Train_dice: 0.4796\n","106/222, Train_loss: 0.5406\n","Train_dice: 0.4850\n","107/222, Train_loss: 0.5367\n","Train_dice: 0.4884\n","108/222, Train_loss: 0.5336\n","Train_dice: 0.4916\n","109/222, Train_loss: 0.5348\n","Train_dice: 0.4900\n","110/222, Train_loss: 0.5393\n","Train_dice: 0.4855\n","111/222, Train_loss: 0.5342\n","Train_dice: 0.4903\n","112/222, Train_loss: 0.5270\n","Train_dice: 0.5001\n","113/222, Train_loss: 0.5262\n","Train_dice: 0.5002\n","114/222, Train_loss: 0.5393\n","Train_dice: 0.4850\n","115/222, Train_loss: 0.5322\n","Train_dice: 0.4949\n","116/222, Train_loss: 0.5309\n","Train_dice: 0.4949\n","117/222, Train_loss: 0.5297\n","Train_dice: 0.4979\n","118/222, Train_loss: 0.5348\n","Train_dice: 0.4891\n","119/222, Train_loss: 0.5360\n","Train_dice: 0.4893\n","120/222, Train_loss: 0.5205\n","Train_dice: 0.5048\n","121/222, Train_loss: 0.5265\n","Train_dice: 0.4991\n","122/222, Train_loss: 0.5096\n","Train_dice: 0.5170\n","123/222, Train_loss: 0.5311\n","Train_dice: 0.4928\n","124/222, Train_loss: 0.5492\n","Train_dice: 0.4731\n","125/222, Train_loss: 0.5474\n","Train_dice: 0.4787\n","126/222, Train_loss: 0.5295\n","Train_dice: 0.4953\n","127/222, Train_loss: 0.5362\n","Train_dice: 0.4870\n","128/222, Train_loss: 0.5137\n","Train_dice: 0.5125\n","129/222, Train_loss: 0.5170\n","Train_dice: 0.5075\n","130/222, Train_loss: 0.5350\n","Train_dice: 0.4886\n","131/222, Train_loss: 0.5216\n","Train_dice: 0.5025\n","132/222, Train_loss: 0.5242\n","Train_dice: 0.5004\n","133/222, Train_loss: 0.5297\n","Train_dice: 0.4933\n","134/222, Train_loss: 0.5332\n","Train_dice: 0.4909\n","135/222, Train_loss: 0.5176\n","Train_dice: 0.5084\n","136/222, Train_loss: 0.5227\n","Train_dice: 0.5008\n","137/222, Train_loss: 0.5321\n","Train_dice: 0.4902\n","138/222, Train_loss: 0.5244\n","Train_dice: 0.5006\n","139/222, Train_loss: 0.5171\n","Train_dice: 0.5069\n","140/222, Train_loss: 0.5248\n","Train_dice: 0.4987\n","141/222, Train_loss: 0.5241\n","Train_dice: 0.4993\n","142/222, Train_loss: 0.5259\n","Train_dice: 0.4972\n","143/222, Train_loss: 0.5190\n","Train_dice: 0.5041\n","144/222, Train_loss: 0.5226\n","Train_dice: 0.5008\n","145/222, Train_loss: 0.5223\n","Train_dice: 0.5001\n","146/222, Train_loss: 0.5310\n","Train_dice: 0.4916\n","147/222, Train_loss: 0.5100\n","Train_dice: 0.5149\n","148/222, Train_loss: 0.5146\n","Train_dice: 0.5096\n","149/222, Train_loss: 0.5367\n","Train_dice: 0.4844\n","150/222, Train_loss: 0.5268\n","Train_dice: 0.4950\n","151/222, Train_loss: 0.5371\n","Train_dice: 0.4853\n","152/222, Train_loss: 0.5188\n","Train_dice: 0.5047\n","153/222, Train_loss: 0.5223\n","Train_dice: 0.5001\n","154/222, Train_loss: 0.5349\n","Train_dice: 0.4864\n","155/222, Train_loss: 0.5219\n","Train_dice: 0.5008\n","156/222, Train_loss: 0.5327\n","Train_dice: 0.4893\n","157/222, Train_loss: 0.5182\n","Train_dice: 0.5038\n","158/222, Train_loss: 0.5231\n","Train_dice: 0.5004\n","159/222, Train_loss: 0.5321\n","Train_dice: 0.4897\n","160/222, Train_loss: 0.5169\n","Train_dice: 0.5053\n","161/222, Train_loss: 0.5384\n","Train_dice: 0.4819\n","162/222, Train_loss: 0.5214\n","Train_dice: 0.4998\n","163/222, Train_loss: 0.5274\n","Train_dice: 0.4933\n","164/222, Train_loss: 0.5150\n","Train_dice: 0.5071\n","165/222, Train_loss: 0.5304\n","Train_dice: 0.4902\n","166/222, Train_loss: 0.5066\n","Train_dice: 0.5154\n","167/222, Train_loss: 0.5192\n","Train_dice: 0.5023\n","168/222, Train_loss: 0.5273\n","Train_dice: 0.4955\n","169/222, Train_loss: 0.5211\n","Train_dice: 0.5000\n","170/222, Train_loss: 0.5096\n","Train_dice: 0.5142\n","171/222, Train_loss: 0.5339\n","Train_dice: 0.4862\n","172/222, Train_loss: 0.5257\n","Train_dice: 0.4947\n","173/222, Train_loss: 0.5363\n","Train_dice: 0.4836\n","174/222, Train_loss: 0.5218\n","Train_dice: 0.5007\n","175/222, Train_loss: 0.5244\n","Train_dice: 0.4961\n","176/222, Train_loss: 0.5151\n","Train_dice: 0.5056\n","177/222, Train_loss: 0.5239\n","Train_dice: 0.4959\n","178/222, Train_loss: 0.5228\n","Train_dice: 0.4999\n","179/222, Train_loss: 0.5231\n","Train_dice: 0.4969\n","180/222, Train_loss: 0.5415\n","Train_dice: 0.4776\n","181/222, Train_loss: 0.5282\n","Train_dice: 0.4927\n","182/222, Train_loss: 0.5035\n","Train_dice: 0.5202\n","183/222, Train_loss: 0.5393\n","Train_dice: 0.4795\n","184/222, Train_loss: 0.5148\n","Train_dice: 0.5053\n","185/222, Train_loss: 0.5008\n","Train_dice: 0.5198\n","186/222, Train_loss: 0.5092\n","Train_dice: 0.5119\n","187/222, Train_loss: 0.5153\n","Train_dice: 0.5050\n","188/222, Train_loss: 0.5056\n","Train_dice: 0.5172\n","189/222, Train_loss: 0.5161\n","Train_dice: 0.5030\n","190/222, Train_loss: 0.5155\n","Train_dice: 0.5051\n","191/222, Train_loss: 0.5449\n","Train_dice: 0.4730\n","192/222, Train_loss: 0.5446\n","Train_dice: 0.4735\n","193/222, Train_loss: 0.5433\n","Train_dice: 0.4743\n","194/222, Train_loss: 0.4980\n","Train_dice: 0.5231\n","195/222, Train_loss: 0.5443\n","Train_dice: 0.4747\n","196/222, Train_loss: 0.5059\n","Train_dice: 0.5145\n","197/222, Train_loss: 0.5359\n","Train_dice: 0.4827\n","198/222, Train_loss: 0.4901\n","Train_dice: 0.5338\n","199/222, Train_loss: 0.5205\n","Train_dice: 0.4990\n","200/222, Train_loss: 0.5236\n","Train_dice: 0.4949\n","201/222, Train_loss: 0.5223\n","Train_dice: 0.4959\n","202/222, Train_loss: 0.5425\n","Train_dice: 0.4747\n","203/222, Train_loss: 0.5419\n","Train_dice: 0.4754\n","204/222, Train_loss: 0.5159\n","Train_dice: 0.5036\n","205/222, Train_loss: 0.5213\n","Train_dice: 0.4972\n","206/222, Train_loss: 0.5011\n","Train_dice: 0.5191\n","207/222, Train_loss: 0.5426\n","Train_dice: 0.4751\n","208/222, Train_loss: 0.5416\n","Train_dice: 0.4754\n","209/222, Train_loss: 0.5440\n","Train_dice: 0.4749\n","210/222, Train_loss: 0.5406\n","Train_dice: 0.4760\n","211/222, Train_loss: 0.5411\n","Train_dice: 0.4755\n","212/222, Train_loss: 0.5407\n","Train_dice: 0.4758\n","213/222, Train_loss: 0.5101\n","Train_dice: 0.5099\n","214/222, Train_loss: 0.5371\n","Train_dice: 0.4890\n","215/222, Train_loss: 0.4969\n","Train_dice: 0.5231\n","216/222, Train_loss: 0.5122\n","Train_dice: 0.5067\n","217/222, Train_loss: 0.5415\n","Train_dice: 0.4759\n","218/222, Train_loss: 0.5167\n","Train_dice: 0.5008\n","219/222, Train_loss: 0.5114\n","Train_dice: 0.5082\n","220/222, Train_loss: 0.5330\n","Train_dice: 0.4844\n","221/222, Train_loss: 0.5407\n","Train_dice: 0.4764\n","222/222, Train_loss: 0.5394\n","Train_dice: 0.4818\n","--------------------\n","Epoch_loss: 0.5330\n","Epoch_metric: 0.4934\n","test_loss_epoch: 0.5095\n","test_dice_epoch: 0.5089\n","Confusion Matrix:\n","[[56918054  1445480]\n"," [   91614   265108]]\n","test_loss_epoch: 0.0091\n","test_dice_epoch: 0.0091\n","current epoch: 3 current mean dice: 0.5035\n","best mean dice: 0.0091 at epoch: 3\n","----------\n","epoch 4/150\n","1/222, Train_loss: 0.5045\n","Train_dice: 0.5130\n","2/222, Train_loss: 0.4954\n","Train_dice: 0.5243\n","3/222, Train_loss: 0.5028\n","Train_dice: 0.5176\n","4/222, Train_loss: 0.5049\n","Train_dice: 0.5130\n","5/222, Train_loss: 0.5053\n","Train_dice: 0.5204\n","6/222, Train_loss: 0.4930\n","Train_dice: 0.5273\n","7/222, Train_loss: 0.5091\n","Train_dice: 0.5103\n","8/222, Train_loss: 0.5030\n","Train_dice: 0.5155\n","9/222, Train_loss: 0.5116\n","Train_dice: 0.5075\n","10/222, Train_loss: 0.5037\n","Train_dice: 0.5140\n","11/222, Train_loss: 0.5031\n","Train_dice: 0.5145\n","12/222, Train_loss: 0.4991\n","Train_dice: 0.5208\n","13/222, Train_loss: 0.4887\n","Train_dice: 0.5319\n","14/222, Train_loss: 0.4775\n","Train_dice: 0.5414\n","15/222, Train_loss: 0.5078\n","Train_dice: 0.5115\n","16/222, Train_loss: 0.5068\n","Train_dice: 0.5106\n","17/222, Train_loss: 0.5113\n","Train_dice: 0.5079\n","18/222, Train_loss: 0.5071\n","Train_dice: 0.5140\n","19/222, Train_loss: 0.5228\n","Train_dice: 0.4944\n","20/222, Train_loss: 0.5220\n","Train_dice: 0.4948\n","21/222, Train_loss: 0.5194\n","Train_dice: 0.4974\n","22/222, Train_loss: 0.4979\n","Train_dice: 0.5229\n","23/222, Train_loss: 0.5040\n","Train_dice: 0.5135\n","24/222, Train_loss: 0.5129\n","Train_dice: 0.5032\n","25/222, Train_loss: 0.5405\n","Train_dice: 0.4779\n","26/222, Train_loss: 0.4881\n","Train_dice: 0.5289\n","27/222, Train_loss: 0.4921\n","Train_dice: 0.5246\n","28/222, Train_loss: 0.5328\n","Train_dice: 0.4829\n","29/222, Train_loss: 0.4971\n","Train_dice: 0.5200\n","30/222, Train_loss: 0.5229\n","Train_dice: 0.4940\n","31/222, Train_loss: 0.5051\n","Train_dice: 0.5108\n","32/222, Train_loss: 0.5213\n","Train_dice: 0.4970\n","33/222, Train_loss: 0.4981\n","Train_dice: 0.5183\n","34/222, Train_loss: 0.4891\n","Train_dice: 0.5277\n","35/222, Train_loss: 0.5017\n","Train_dice: 0.5143\n","36/222, Train_loss: 0.4827\n","Train_dice: 0.5348\n","37/222, Train_loss: 0.4830\n","Train_dice: 0.5353\n","38/222, Train_loss: 0.5028\n","Train_dice: 0.5124\n","39/222, Train_loss: 0.4807\n","Train_dice: 0.5358\n","40/222, Train_loss: 0.4918\n","Train_dice: 0.5232\n","41/222, Train_loss: 0.5259\n","Train_dice: 0.4890\n","42/222, Train_loss: 0.4583\n","Train_dice: 0.5635\n","43/222, Train_loss: 0.4811\n","Train_dice: 0.5344\n","44/222, Train_loss: 0.5339\n","Train_dice: 0.4808\n","45/222, Train_loss: 0.5233\n","Train_dice: 0.4948\n","46/222, Train_loss: 0.4938\n","Train_dice: 0.5210\n","47/222, Train_loss: 0.5242\n","Train_dice: 0.4906\n","48/222, Train_loss: 0.4964\n","Train_dice: 0.5198\n","49/222, Train_loss: 0.5021\n","Train_dice: 0.5141\n","50/222, Train_loss: 0.5142\n","Train_dice: 0.5009\n","51/222, Train_loss: 0.5356\n","Train_dice: 0.4791\n","52/222, Train_loss: 0.5022\n","Train_dice: 0.5124\n","53/222, Train_loss: 0.4831\n","Train_dice: 0.5311\n","54/222, Train_loss: 0.4925\n","Train_dice: 0.5248\n","55/222, Train_loss: 0.4836\n","Train_dice: 0.5320\n","56/222, Train_loss: 0.5370\n","Train_dice: 0.4775\n","57/222, Train_loss: 0.4859\n","Train_dice: 0.5292\n","58/222, Train_loss: 0.4890\n","Train_dice: 0.5265\n","59/222, Train_loss: 0.5361\n","Train_dice: 0.4780\n","60/222, Train_loss: 0.4980\n","Train_dice: 0.5177\n","61/222, Train_loss: 0.4811\n","Train_dice: 0.5339\n","62/222, Train_loss: 0.4997\n","Train_dice: 0.5253\n","63/222, Train_loss: 0.5355\n","Train_dice: 0.4780\n","64/222, Train_loss: 0.4907\n","Train_dice: 0.5245\n","65/222, Train_loss: 0.4874\n","Train_dice: 0.5267\n","66/222, Train_loss: 0.4863\n","Train_dice: 0.5274\n","67/222, Train_loss: 0.4898\n","Train_dice: 0.5254\n","68/222, Train_loss: 0.4924\n","Train_dice: 0.5218\n","69/222, Train_loss: 0.5001\n","Train_dice: 0.5147\n","70/222, Train_loss: 0.4851\n","Train_dice: 0.5304\n","71/222, Train_loss: 0.5027\n","Train_dice: 0.5116\n","72/222, Train_loss: 0.4792\n","Train_dice: 0.5345\n","73/222, Train_loss: 0.5014\n","Train_dice: 0.5135\n","74/222, Train_loss: 0.5035\n","Train_dice: 0.5104\n","75/222, Train_loss: 0.4956\n","Train_dice: 0.5180\n","76/222, Train_loss: 0.5130\n","Train_dice: 0.5005\n","77/222, Train_loss: 0.4999\n","Train_dice: 0.5132\n","78/222, Train_loss: 0.4958\n","Train_dice: 0.5222\n","79/222, Train_loss: 0.5034\n","Train_dice: 0.5103\n","80/222, Train_loss: 0.5020\n","Train_dice: 0.5113\n","81/222, Train_loss: 0.5020\n","Train_dice: 0.5105\n","82/222, Train_loss: 0.5034\n","Train_dice: 0.5101\n","83/222, Train_loss: 0.4885\n","Train_dice: 0.5251\n","84/222, Train_loss: 0.5013\n","Train_dice: 0.5117\n","85/222, Train_loss: 0.4998\n","Train_dice: 0.5166\n","86/222, Train_loss: 0.5170\n","Train_dice: 0.4965\n","87/222, Train_loss: 0.5066\n","Train_dice: 0.5069\n","88/222, Train_loss: 0.4696\n","Train_dice: 0.5427\n","89/222, Train_loss: 0.4998\n","Train_dice: 0.5143\n","90/222, Train_loss: 0.5112\n","Train_dice: 0.5018\n","91/222, Train_loss: 0.5023\n","Train_dice: 0.5103\n","92/222, Train_loss: 0.4800\n","Train_dice: 0.5324\n","93/222, Train_loss: 0.4972\n","Train_dice: 0.5160\n","94/222, Train_loss: 0.5015\n","Train_dice: 0.5112\n","95/222, Train_loss: 0.5101\n","Train_dice: 0.5025\n","96/222, Train_loss: 0.4899\n","Train_dice: 0.5225\n","97/222, Train_loss: 0.4992\n","Train_dice: 0.5132\n","98/222, Train_loss: 0.5162\n","Train_dice: 0.4969\n","99/222, Train_loss: 0.4989\n","Train_dice: 0.5137\n","100/222, Train_loss: 0.5052\n","Train_dice: 0.5077\n","101/222, Train_loss: 0.4920\n","Train_dice: 0.5201\n","102/222, Train_loss: 0.4928\n","Train_dice: 0.5196\n","103/222, Train_loss: 0.4768\n","Train_dice: 0.5362\n","104/222, Train_loss: 0.5320\n","Train_dice: 0.4803\n","105/222, Train_loss: 0.5168\n","Train_dice: 0.4950\n","106/222, Train_loss: 0.5100\n","Train_dice: 0.5022\n","107/222, Train_loss: 0.5029\n","Train_dice: 0.5085\n","108/222, Train_loss: 0.5002\n","Train_dice: 0.5116\n","109/222, Train_loss: 0.5016\n","Train_dice: 0.5098\n","110/222, Train_loss: 0.5097\n","Train_dice: 0.5023\n","111/222, Train_loss: 0.5006\n","Train_dice: 0.5108\n","112/222, Train_loss: 0.4877\n","Train_dice: 0.5249\n","113/222, Train_loss: 0.4895\n","Train_dice: 0.5232\n","114/222, Train_loss: 0.5106\n","Train_dice: 0.5013\n","115/222, Train_loss: 0.4984\n","Train_dice: 0.5158\n","116/222, Train_loss: 0.4965\n","Train_dice: 0.5163\n","117/222, Train_loss: 0.4876\n","Train_dice: 0.5245\n","118/222, Train_loss: 0.5036\n","Train_dice: 0.5076\n","119/222, Train_loss: 0.5013\n","Train_dice: 0.5102\n","120/222, Train_loss: 0.4837\n","Train_dice: 0.5278\n","121/222, Train_loss: 0.4896\n","Train_dice: 0.5215\n","122/222, Train_loss: 0.4669\n","Train_dice: 0.5447\n","123/222, Train_loss: 0.4973\n","Train_dice: 0.5135\n","124/222, Train_loss: 0.5259\n","Train_dice: 0.4852\n","125/222, Train_loss: 0.5241\n","Train_dice: 0.4918\n","126/222, Train_loss: 0.4992\n","Train_dice: 0.5133\n","127/222, Train_loss: 0.5059\n","Train_dice: 0.5048\n","128/222, Train_loss: 0.4713\n","Train_dice: 0.5407\n","129/222, Train_loss: 0.4757\n","Train_dice: 0.5342\n","130/222, Train_loss: 0.5061\n","Train_dice: 0.5052\n","131/222, Train_loss: 0.4834\n","Train_dice: 0.5265\n","132/222, Train_loss: 0.4866\n","Train_dice: 0.5240\n","133/222, Train_loss: 0.4976\n","Train_dice: 0.5128\n","134/222, Train_loss: 0.5017\n","Train_dice: 0.5096\n","135/222, Train_loss: 0.4778\n","Train_dice: 0.5341\n","136/222, Train_loss: 0.4854\n","Train_dice: 0.5242\n","137/222, Train_loss: 0.5017\n","Train_dice: 0.5081\n","138/222, Train_loss: 0.4874\n","Train_dice: 0.5244\n","139/222, Train_loss: 0.4769\n","Train_dice: 0.5329\n","140/222, Train_loss: 0.4904\n","Train_dice: 0.5197\n","141/222, Train_loss: 0.4891\n","Train_dice: 0.5208\n","142/222, Train_loss: 0.4909\n","Train_dice: 0.5191\n","143/222, Train_loss: 0.4811\n","Train_dice: 0.5280\n","144/222, Train_loss: 0.4870\n","Train_dice: 0.5231\n","145/222, Train_loss: 0.4867\n","Train_dice: 0.5221\n","146/222, Train_loss: 0.4992\n","Train_dice: 0.5107\n","147/222, Train_loss: 0.4678\n","Train_dice: 0.5428\n","148/222, Train_loss: 0.4763\n","Train_dice: 0.5338\n","149/222, Train_loss: 0.5095\n","Train_dice: 0.4998\n","150/222, Train_loss: 0.4945\n","Train_dice: 0.5145\n","151/222, Train_loss: 0.5145\n","Train_dice: 0.4973\n","152/222, Train_loss: 0.4800\n","Train_dice: 0.5296\n","153/222, Train_loss: 0.4884\n","Train_dice: 0.5213\n","154/222, Train_loss: 0.5069\n","Train_dice: 0.5027\n","155/222, Train_loss: 0.4874\n","Train_dice: 0.5225\n","156/222, Train_loss: 0.5034\n","Train_dice: 0.5062\n","157/222, Train_loss: 0.4821\n","Train_dice: 0.5266\n","158/222, Train_loss: 0.4884\n","Train_dice: 0.5224\n","159/222, Train_loss: 0.5030\n","Train_dice: 0.5072\n","160/222, Train_loss: 0.4806\n","Train_dice: 0.5283\n","161/222, Train_loss: 0.5131\n","Train_dice: 0.4965\n","162/222, Train_loss: 0.4895\n","Train_dice: 0.5194\n","163/222, Train_loss: 0.4974\n","Train_dice: 0.5117\n","164/222, Train_loss: 0.4761\n","Train_dice: 0.5325\n","165/222, Train_loss: 0.5024\n","Train_dice: 0.5067\n","166/222, Train_loss: 0.4653\n","Train_dice: 0.5422\n","167/222, Train_loss: 0.4844\n","Train_dice: 0.5243\n","168/222, Train_loss: 0.4915\n","Train_dice: 0.5184\n","169/222, Train_loss: 0.4861\n","Train_dice: 0.5221\n","170/222, Train_loss: 0.4659\n","Train_dice: 0.5436\n","171/222, Train_loss: 0.5076\n","Train_dice: 0.5017\n","172/222, Train_loss: 0.4947\n","Train_dice: 0.5140\n","173/222, Train_loss: 0.5114\n","Train_dice: 0.4980\n","174/222, Train_loss: 0.4879\n","Train_dice: 0.5219\n","175/222, Train_loss: 0.4949\n","Train_dice: 0.5140\n","176/222, Train_loss: 0.4809\n","Train_dice: 0.5272\n","177/222, Train_loss: 0.4934\n","Train_dice: 0.5145\n","178/222, Train_loss: 0.4840\n","Train_dice: 0.5256\n","179/222, Train_loss: 0.4949\n","Train_dice: 0.5141\n","180/222, Train_loss: 0.5212\n","Train_dice: 0.4886\n","181/222, Train_loss: 0.4983\n","Train_dice: 0.5118\n","182/222, Train_loss: 0.4597\n","Train_dice: 0.5497\n","183/222, Train_loss: 0.5171\n","Train_dice: 0.4922\n","184/222, Train_loss: 0.4791\n","Train_dice: 0.5285\n","185/222, Train_loss: 0.4579\n","Train_dice: 0.5485\n","186/222, Train_loss: 0.4695\n","Train_dice: 0.5380\n","187/222, Train_loss: 0.4835\n","Train_dice: 0.5253\n","188/222, Train_loss: 0.4660\n","Train_dice: 0.5437\n","189/222, Train_loss: 0.4823\n","Train_dice: 0.5243\n","190/222, Train_loss: 0.4790\n","Train_dice: 0.5290\n","191/222, Train_loss: 0.5266\n","Train_dice: 0.4825\n","192/222, Train_loss: 0.5268\n","Train_dice: 0.4823\n","193/222, Train_loss: 0.5253\n","Train_dice: 0.4836\n","194/222, Train_loss: 0.4526\n","Train_dice: 0.5538\n","195/222, Train_loss: 0.5266\n","Train_dice: 0.4837\n","196/222, Train_loss: 0.4651\n","Train_dice: 0.5425\n","197/222, Train_loss: 0.5120\n","Train_dice: 0.4965\n","198/222, Train_loss: 0.4438\n","Train_dice: 0.5661\n","199/222, Train_loss: 0.4914\n","Train_dice: 0.5172\n","200/222, Train_loss: 0.4940\n","Train_dice: 0.5135\n","201/222, Train_loss: 0.4938\n","Train_dice: 0.5133\n","202/222, Train_loss: 0.5259\n","Train_dice: 0.4831\n","203/222, Train_loss: 0.5258\n","Train_dice: 0.4834\n","204/222, Train_loss: 0.4893\n","Train_dice: 0.5198\n","205/222, Train_loss: 0.4874\n","Train_dice: 0.5191\n","206/222, Train_loss: 0.4616\n","Train_dice: 0.5449\n","207/222, Train_loss: 0.5250\n","Train_dice: 0.4839\n","208/222, Train_loss: 0.5249\n","Train_dice: 0.4842\n","209/222, Train_loss: 0.5257\n","Train_dice: 0.4842\n","210/222, Train_loss: 0.5238\n","Train_dice: 0.4846\n","211/222, Train_loss: 0.5248\n","Train_dice: 0.4839\n","212/222, Train_loss: 0.5241\n","Train_dice: 0.4846\n","213/222, Train_loss: 0.4783\n","Train_dice: 0.5314\n","214/222, Train_loss: 0.5139\n","Train_dice: 0.5053\n","215/222, Train_loss: 0.4533\n","Train_dice: 0.5529\n","216/222, Train_loss: 0.4735\n","Train_dice: 0.5329\n","217/222, Train_loss: 0.5292\n","Train_dice: 0.4835\n","218/222, Train_loss: 0.4856\n","Train_dice: 0.5209\n","219/222, Train_loss: 0.4807\n","Train_dice: 0.5287\n","220/222, Train_loss: 0.5135\n","Train_dice: 0.4953\n","221/222, Train_loss: 0.5244\n","Train_dice: 0.4844\n","222/222, Train_loss: 0.5253\n","Train_dice: 0.4897\n","--------------------\n","Epoch_loss: 0.4980\n","Epoch_metric: 0.5147\n","test_loss_epoch: 0.4788\n","test_dice_epoch: 0.5270\n","Confusion Matrix:\n","[[56656517  1707017]\n"," [   73666   283056]]\n","test_loss_epoch: 0.0086\n","test_dice_epoch: 0.0094\n","current epoch: 4 current mean dice: 0.5257\n","best mean dice: 0.0094 at epoch: 4\n","----------\n","epoch 5/150\n","1/222, Train_loss: 0.4668\n","Train_dice: 0.5380\n","2/222, Train_loss: 0.4540\n","Train_dice: 0.5529\n","3/222, Train_loss: 0.4603\n","Train_dice: 0.5463\n","4/222, Train_loss: 0.4675\n","Train_dice: 0.5373\n","5/222, Train_loss: 0.4580\n","Train_dice: 0.5550\n","6/222, Train_loss: 0.4510\n","Train_dice: 0.5560\n","7/222, Train_loss: 0.4753\n","Train_dice: 0.5327\n","8/222, Train_loss: 0.4664\n","Train_dice: 0.5402\n","9/222, Train_loss: 0.4740\n","Train_dice: 0.5327\n","10/222, Train_loss: 0.4681\n","Train_dice: 0.5374\n","11/222, Train_loss: 0.4669\n","Train_dice: 0.5389\n","12/222, Train_loss: 0.4566\n","Train_dice: 0.5505\n","13/222, Train_loss: 0.4490\n","Train_dice: 0.5594\n","14/222, Train_loss: 0.4267\n","Train_dice: 0.5770\n","15/222, Train_loss: 0.4676\n","Train_dice: 0.5386\n","16/222, Train_loss: 0.4676\n","Train_dice: 0.5372\n","17/222, Train_loss: 0.4757\n","Train_dice: 0.5323\n","18/222, Train_loss: 0.4746\n","Train_dice: 0.5368\n","19/222, Train_loss: 0.4918\n","Train_dice: 0.5144\n","20/222, Train_loss: 0.4959\n","Train_dice: 0.5110\n","21/222, Train_loss: 0.4882\n","Train_dice: 0.5176\n","22/222, Train_loss: 0.4608\n","Train_dice: 0.5492\n","23/222, Train_loss: 0.4664\n","Train_dice: 0.5387\n","24/222, Train_loss: 0.4810\n","Train_dice: 0.5242\n","25/222, Train_loss: 0.5241\n","Train_dice: 0.4872\n","26/222, Train_loss: 0.4470\n","Train_dice: 0.5572\n","27/222, Train_loss: 0.4527\n","Train_dice: 0.5517\n","28/222, Train_loss: 0.5133\n","Train_dice: 0.4942\n","29/222, Train_loss: 0.4577\n","Train_dice: 0.5468\n","30/222, Train_loss: 0.4968\n","Train_dice: 0.5115\n","31/222, Train_loss: 0.4688\n","Train_dice: 0.5354\n","32/222, Train_loss: 0.4989\n","Train_dice: 0.5112\n","33/222, Train_loss: 0.4590\n","Train_dice: 0.5446\n","34/222, Train_loss: 0.4515\n","Train_dice: 0.5532\n","35/222, Train_loss: 0.4663\n","Train_dice: 0.5381\n","36/222, Train_loss: 0.4378\n","Train_dice: 0.5646\n","37/222, Train_loss: 0.4374\n","Train_dice: 0.5673\n","38/222, Train_loss: 0.4718\n","Train_dice: 0.5323\n","39/222, Train_loss: 0.4336\n","Train_dice: 0.5687\n","40/222, Train_loss: 0.4527\n","Train_dice: 0.5497\n","41/222, Train_loss: 0.5034\n","Train_dice: 0.5026\n","42/222, Train_loss: 0.4010\n","Train_dice: 0.6059\n","43/222, Train_loss: 0.4386\n","Train_dice: 0.5641\n","44/222, Train_loss: 0.5160\n","Train_dice: 0.4909\n","45/222, Train_loss: 0.4907\n","Train_dice: 0.5164\n","46/222, Train_loss: 0.4516\n","Train_dice: 0.5506\n","47/222, Train_loss: 0.4993\n","Train_dice: 0.5066\n","48/222, Train_loss: 0.4568\n","Train_dice: 0.5472\n","49/222, Train_loss: 0.4738\n","Train_dice: 0.5331\n","50/222, Train_loss: 0.4858\n","Train_dice: 0.5192\n","51/222, Train_loss: 0.5191\n","Train_dice: 0.4878\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-cc4e2c621e10>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-ad13d4d9ab87>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_in, loss, optim, max_epochs, model_dir, test_interval, device)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mtrain_epoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             print(\n\u001b[1;32m     67\u001b[0m                 \u001b[0;34mf\"{train_step}/{len(train_loader) // train_loader.batch_size}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/monai/data/meta_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;31m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# if \"out\" in kwargs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_default_nowrap_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","from torch import nn\n","from monai.losses import DiceLoss\n","from sklearn.metrics import confusion_matrix\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","args = {\n","    'model_name': 'UNet',\n","    'pretrained': False,\n","    'dropout': 0.2\n","}\n","model = get_model(args)\n","model = model.to(device)\n","\n","# loss_function = nn.CrossEntropyLoss().to(device)\n","loss_function = DiceLoss(to_onehot_y=True, softmax=True).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), 1e-4, amsgrad=True)\n","\n","if __name__ == '__main__':\n","    train(model, data_in, loss_function, optimizer, 150, model_dir)"]},{"cell_type":"markdown","metadata":{"id":"dBejD7L32MIz"},"source":["## Check the Size of Data (Add to main) - For Debugging"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qe9HMsaw3pk9"},"outputs":[],"source":["# assuming prepare function has been imported from another file\n","train_loader, test_loader = prepare(data_dir)\n","\n","for batch_data in  train_loader:\n","  volume = batch_data[\"vol\"]\n","  label = batch_data[\"seg\"]\n","\n","  # print size of volume and label tensors\n","  print(f\"Volume size: {volume.size()}\")\n","  print(f\"Label size: {label.size()}\")\n","\n","  # only process one batch\n","  break"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}